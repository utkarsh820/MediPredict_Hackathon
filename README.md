
---

# ğŸ©º Disease Prediction using ML (Mini Hackathon)

## ğŸ“Œ Overview

This project is part of a **mini-hackathon challenge** where the goal is to build a machine learning model that can predict the presence of various diseases based on patient symptoms.

We trained and compared three state-of-the-art models:

* âš¡ **XGBoost**
* ğŸŒ¿ **LightGBM**
* ğŸ± **CatBoost**

The pipeline includes preprocessing, training, saving models, and evaluating them on a held-out test dataset.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ cleaned_data.csv      # Training dataset
â”‚   â”œâ”€â”€ Testing.csv           # Testing dataset
â”‚
â”œâ”€â”€ saved_models/
â”‚   â”œâ”€â”€ xgb_disease_model.pkl # Trained XGBoost model
â”‚   â”œâ”€â”€ lgb_disease_model.pkl # Trained LightGBM model
â”‚   â”œâ”€â”€ cat_disease_model.pkl # Trained CatBoost model
â”‚   â”œâ”€â”€ label_encoder.pkl     # Fitted label encoder for diseases
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_models.py       # Train and save XGB, LGBM, CatBoost
â”‚   â”œâ”€â”€ test_single_model.py  # Load one model + evaluate
â”‚   â”œâ”€â”€ compare_models.py     # Load all models + compare accuracy
â”‚
â”œâ”€â”€ README.md                 # Project documentation
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone repo & install dependencies

```bash
git clone <repo_url>
cd <repo_name>
pip install -r requirements.txt
```

Typical requirements:

```txt
pandas
scikit-learn
xgboost
lightgbm
catboost
matplotlib
joblib
```

---

### 2ï¸âƒ£ Train Models

Run training script to build and save all three models:

```bash
python scripts/train_models.py
```

This will:

* Train **XGBoost, LightGBM, and CatBoost** on `cleaned_data.csv`
* Save trained models into `saved_models/`

---

### 3ï¸âƒ£ Test a Single Model

To test an individual model (example: XGBoost):

```bash
python scripts/test_single_model.py
```

It will:

* Load model from `saved_models/`
* Evaluate on `Testing.csv`
* Print accuracy + classification report

---

### 4ï¸âƒ£ Compare All Models

To compare **XGBoost, LightGBM, and CatBoost** side by side:

```bash
python scripts/compare_models.py
```

It will:

* Print accuracy + classification report for each model
* Show a **bar chart** comparing accuracy

---

## ğŸ“Š Example Results

| Model    | Accuracy |
| -------- | -------- |
| XGBoost  | 0.95     |
| LightGBM | 0.96     |
| CatBoost | 0.97     |

*(Values will vary depending on dataset & random seed)*

---

## ğŸ“Œ Key Notes

* The target variable is **`prognosis`** (disease label).
* Labels are encoded using `LabelEncoder` before training.
* All models are saved with **joblib** for easy loading.

---

## ğŸš€ Future Improvements

* Add **confusion matrices** for better error analysis
* Hyperparameter tuning using **Optuna**
* Deploy model via **FastAPI/Streamlit** for interactive predictions

---
